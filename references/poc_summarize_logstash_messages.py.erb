#!/usr/bin/env python3
import sys
import json
import logging
import re
from drain3 import TemplateMiner
from drain3.template_miner_config import TemplateMinerConfig
from drain3.masking import MaskingInstruction

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(message)s')

# Check arguments
if len(sys.argv) != 3:
    print(f"Usage: {sys.argv[0]} <input_file> <output_file>")
    sys.exit(1)

input_file = sys.argv[1]
output_file = sys.argv[2]

# Configure the template miner with custom masking
config = TemplateMinerConfig()
config.mask_prefix = "<"
config.mask_suffix = ">"

# Clear default masking instructions and add custom ones
config.masking_instructions = []

# Add custom masking instructions
# 1. IP addresses
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}',
        mask_with='IP'
    )
)

# 2. Timestamps in various formats
# Timestamps enclosed in square brackets (common in log formats)
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\[\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?(\+|-)\d{2}:\d{2}\]',
        mask_with='[TIMESTAMP]'
    )
)
# Alternative format with space instead of 'T'
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(\.\d+)?\]',
        mask_with='[TIMESTAMP]'
    )
)
# ISO 8601 timestamps
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?(\+|-)\d{2}:\d{2}',
        mask_with='TIMESTAMP'
    )
)
# Common log format timestamps
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\d{2}/[A-Za-z]{3}/\d{4}:\d{2}:\d{2}:\d{2} (\+|-)\d{4}',
        mask_with='TIMESTAMP'
    )
)
# Simple timestamps
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'[A-Z][a-z]{2} \d{1,2} \d{2}:\d{2}:\d{2}',
        mask_with='TIMESTAMP'
    )
)
# Date-only formats
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\d{4}-\d{2}-\d{2}',
        mask_with='DATE'
    )
)
# Time-only formats
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\d{2}:\d{2}:\d{2}(\.\d+)?',
        mask_with='TIME'
    )
)

# 3. UUIDs
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}',
        mask_with='UUID'
    )
)

# 4. Hexadecimal identifiers (but not in URLs)
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\b[0-9a-f]{16,}\b',
        mask_with='HEX'
    )
)

# 5. Process IDs
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\[\d+\]',
        mask_with='[PID]'
    )
)

# 6. Line numbers in stack traces (but not in file paths)
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'line:? \d+',
        mask_with='line: NUM'
    )
)
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r':\d+\)',
        mask_with=':NUM)'
    )
)

# 7. Query parameters in URLs (but keep the path)
config.masking_instructions.append(
    MaskingInstruction(
        pattern=r'\?[^"\'<>\s]*',
        mask_with='?PARAMS'
    )
)

# 8. Numbers in error messages (but not in URLs)
# We'll handle this with a custom pre-processing step

# Create the template miner with our custom config
template_miner = TemplateMiner(config=config)

# Function to normalize JSON objects
def normalize_json(json_obj):
    """
    Recursively process a JSON object and mask variable parts while preserving structure.
    """
    if isinstance(json_obj, dict):
        result = {}
        for key, value in json_obj.items():
            # Keep the keys as is, normalize the values
            result[key] = normalize_json(value)
        return result
    elif isinstance(json_obj, list):
        # For lists, normalize each element
        return [normalize_json(item) for item in json_obj]
    elif isinstance(json_obj, str):
        # Mask UUIDs
        if re.match(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', json_obj):
            return "<UUID>"
        # Mask timestamps
        elif re.match(r'^\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?(\+|-)\d{2}:\d{2}$', json_obj):
            return "<TIMESTAMP>"
        # Mask emails
        elif re.match(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$', json_obj):
            return "<EMAIL>"
        # For person names, we could add specific logic, but for now we'll keep them
        # For other strings, keep them as is
        return json_obj
    elif isinstance(json_obj, int) or isinstance(json_obj, float):
        # Mask numbers
        return "<NUM>"
    else:
        # For booleans, null, etc., keep them as is
        return json_obj

# Define a function to identify and protect file paths
def protect_file_paths(line):
    """
    Identify and protect file paths in log messages.
    Replace line numbers in file paths with <NUM> but keep the path structure.
    """
    # Common file path patterns in error messages
    file_path_patterns = [
        # Pattern for PHP errors: in /path/to/file.php on line 123
        r'(in\s+)(/[^\s:]+)(\s+on\s+line\s+)(\d+)',
        # Pattern for stack traces: at /path/to/file.php:123
        r'(at\s+)(/[^\s:]+):(\d+)',
        # Pattern for file paths with line numbers: /path/to/file.php:123
        r'(^|\s)(/[^\s:]+):(\d+)(\s|$)',
        # Pattern for file paths in quotes: '/path/to/file.php'
        r'[\'"](/[^\'"]+)[\'"]'
    ]
    
    for pattern in file_path_patterns:
        if 'on line' in pattern:
            # For PHP-style errors: "in /path/to/file.php on line 123"
            line = re.sub(pattern, r'\1\2\3<NUM>', line)
        elif 'at' in pattern:
            # For stack traces: "at /path/to/file.php:123"
            line = re.sub(pattern, r'\1\2:<NUM>', line)
        elif ':' in pattern:
            # For general file paths with line numbers: "/path/to/file.php:123"
            line = re.sub(pattern, r'\1\2:<NUM>\4', line)
        else:
            # For file paths in quotes: don't modify
            pass
    
    return line

# Define a custom pre-processing function to handle URLs and numbers
def preprocess_log_line(line):
    # First, identify and temporarily mark timestamp patterns in square brackets
    # This needs to be done before general number replacement
    timestamp_pattern = r'\[\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?(\+|-)\d{2}:\d{2}\]'
    line = re.sub(timestamp_pattern, '[TIMESTAMP]', line)
    
    # Alternative format with space instead of 'T'
    alt_timestamp_pattern = r'\[\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}(\.\d+)?\]'
    line = re.sub(alt_timestamp_pattern, '[TIMESTAMP]', line)
    
    # Handle other common timestamp formats
    iso_timestamp = r'\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}(\.\d+)?(\+|-)\d{2}:\d{2}'
    line = re.sub(iso_timestamp, 'TIMESTAMP', line)
    
    # Identify and protect file paths (normalize line numbers but keep paths)
    line = protect_file_paths(line)
    
    # Handle JSON structures in log messages
    # Find JSON objects enclosed in curly braces
    json_pattern = r'(\{.*\})'
    json_matches = re.findall(json_pattern, line)
    
    for json_str in json_matches:
        try:
            # Try to parse the JSON
            parsed_json = json.loads(json_str)
            
            # Create a normalized version with masked values but preserved structure
            normalized_json = normalize_json(parsed_json)
            
            # Replace the original JSON with the normalized version
            line = line.replace(json_str, json.dumps(normalized_json))
        except json.JSONDecodeError:
            # If it's not valid JSON, continue with normal processing
            pass
    
    # Identify and temporarily mark HTTP verb + URL patterns
    http_pattern = r'(GET|POST|PUT|DELETE|PATCH|HEAD|OPTIONS) ([^ ]+) HTTP/\d+\.\d+'
    
    # Function to replace numbers in a string, but preserve URL paths
    def replace_numbers_except_in_urls(match):
        verb = match.group(1)
        url = match.group(2)
        http_version = match.group(0).split(' ')[-1]
        
        # Replace numbers in the HTTP version
        http_version = re.sub(r'\d+', '<NUM>', http_version)
        
        # Keep the URL path intact, but mask query parameters
        url_parts = url.split('?', 1)
        path = url_parts[0]
        
        # If there are query parameters, mask them
        if len(url_parts) > 1:
            return f'{verb} {path}<?PARAMS> {http_version}'
        else:
            return f'{verb} {path} {http_version}'
    
    # Apply the URL handling
    line = re.sub(http_pattern, replace_numbers_except_in_urls, line)
    
    # Now replace numbers in the rest of the line (not in URLs or file paths)
    # This regex avoids replacing numbers that are part of file paths
    line = re.sub(r'(?<![a-zA-Z0-9/_.-])(\d+)(?![a-zA-Z0-9/_.-])', '<NUM>', line)
    line = re.sub(r'(?<![a-zA-Z0-9/_.-])(\d+\.\d+)(?![a-zA-Z0-9/_.-])', '<NUM>.<NUM>', line)
    
    return line

# Dictionary to store document IDs for each template
pattern_doc_ids = {}

# Process the input file
print(f"Processing {input_file}...")
with open(input_file, 'r') as f:
    for line in f:
        line = line.strip()
        if not line:
            continue
        
        # Split the line into document ID part and message part
        parts = line.split('|', 1)
        if len(parts) != 2:
            continue
            
        doc_id_part, message = parts
        
        # Apply our custom pre-processing to the message part only
        processed_message = preprocess_log_line(message)
        
        # Add to template miner
        result = template_miner.add_log_message(processed_message)
        
        # Store the document ID with its template
        template = result["cluster_id"]
        if template not in pattern_doc_ids:
            pattern_doc_ids[template] = []
        
        # Add the doc ID to the list, keeping only the 5 most recent
        pattern_doc_ids[template].append(doc_id_part)
        if len(pattern_doc_ids[template]) > 5:
            pattern_doc_ids[template].pop(0)  # Remove the oldest ID

# Get all clusters and write to output file
clusters = []
for cluster in template_miner.drain.clusters:
    # Post-process the template to make it more readable
    template = cluster.get_template()
    
    # Replace consecutive masked parameters with a single mask
    template = re.sub(r'(<[^>]+>)(\s*\1)+', r'\1', template)
    
    # Ensure timestamp components are consistently masked
    # Replace patterns like <NUM>:<NUM>:<NUM> with <TIME>
    template = re.sub(r'<NUM>:<NUM>:<NUM>(\.<NUM>)?', '<TIME>', template)
    
    # Replace patterns like <NUM>-<NUM>-<NUM> with <DATE>
    template = re.sub(r'<NUM>-<NUM>-<NUM>', '<DATE>', template)
    
    # Replace patterns like [<DATE>T<TIME>+<NUM>:<NUM>] with [<TIMESTAMP>]
    template = re.sub(r'\[<DATE>T<TIME>(\+|-)<NUM>:<NUM>\]', '[<TIMESTAMP>]', template)
    
    # Handle alternative format with space instead of 'T'
    template = re.sub(r'\[<DATE> <TIME>\]', '[<TIMESTAMP>]', template)
    
    # Handle any remaining timestamp-like patterns
    template = re.sub(r'\[<NUM>-<NUM>-<NUM>T?<NUM>:<NUM>:<NUM>(.<NUM>)?(\+|-)?<NUM>?:?<NUM>?\]', '[<TIMESTAMP>]', template)
    
    # Ensure file paths are preserved (don't replace them with <*>)
    # This is handled by our preprocessing, but we need to make sure Drain3 doesn't replace them
    
    clusters.append({
        "size": cluster.size,
        "template": template,
        "doc_ids": pattern_doc_ids.get(cluster.cluster_id, [])
    })

# Sort clusters by size (descending)
sorted_clusters = sorted(clusters, key=lambda x: x["size"], reverse=True)

# Write the results to the output file
with open(output_file, 'w') as out:
    for cluster in sorted_clusters:
        doc_ids_str = ""
        if cluster["doc_ids"]:
            doc_ids_str = f"|sample_doc_ids:{','.join(cluster['doc_ids'])}"
        out.write(f"{cluster['size']}|{cluster['template']}{doc_ids_str}\n")

print(f"Processed {len(template_miner.drain.clusters)} unique patterns.")
print(f"Summarized log patterns written to {output_file}")
